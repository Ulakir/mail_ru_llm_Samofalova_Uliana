{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kUbZTtr_QasH"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import pickle\n",
        "from itertools import chain\n",
        "from datetime import datetime\n",
        "from collections import defaultdict\n",
        "\n",
        "from typing import List, Dict, Optional, Iterable, Tuple\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "import tokenizers\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.processors import TemplateProcessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CTeb-4dmQkbH"
      },
      "outputs": [],
      "source": [
        "class Tokenizer:\n",
        "    def __init__(self,\n",
        "                 token_pattern: str = '\\w+|[\\!\\?\\,\\.\\-\\:]',\n",
        "                 eos_token: str = '<EOS>',\n",
        "                 pad_token: str = '<PAD>',\n",
        "                 unk_token: str = '<UNK>'):\n",
        "        self.token_pattern = token_pattern\n",
        "        self.eos_token = eos_token\n",
        "        self.pad_token = pad_token\n",
        "        self.unk_token = unk_token\n",
        "\n",
        "        self.special_tokens = [self.eos_token, self.pad_token, self.unk_token]\n",
        "        self.vocab = None\n",
        "        self.inverse_vocab = None\n",
        "\n",
        "    def text_preprocess(self, input_text: str) -> str:\n",
        "        \"\"\" Предобрабатываем один текст \"\"\"\n",
        "        # input_text = ... # приведение к нижнему регистру\n",
        "        input_text = input_text.lower()\n",
        "        input_text = re.sub('\\s+', ' ', input_text) # унифицируем пробелы\n",
        "        input_text = input_text.strip()\n",
        "        return input_text\n",
        "\n",
        "    def build_vocab(self, corpus: List[str]) -> None:\n",
        "        assert len(corpus)\n",
        "        all_tokens = set()\n",
        "        for text in corpus:\n",
        "            all_tokens |= set(self._tokenize(text, append_eos_token=False))\n",
        "        self.vocab = {elem: ind for ind, elem in enumerate(all_tokens)}\n",
        "        special_tokens = [self.eos_token, self.unk_token, self.pad_token]\n",
        "        for token in special_tokens:\n",
        "            self.vocab[token] = len(self.vocab)\n",
        "        self.inverse_vocab = {ind: elem for elem, ind in self.vocab.items()}\n",
        "        return self\n",
        "\n",
        "    def _tokenize(self, text: str, append_eos_token: bool = True) -> List[str]:\n",
        "        text = self.text_preprocess(text)\n",
        "        tokens = re.findall(self.token_pattern, text)\n",
        "        if append_eos_token:\n",
        "            tokens.append(self.eos_token)\n",
        "        return tokens\n",
        "\n",
        "    def encode(self, text: str, append_eos_token: bool = True) -> List[str]:\n",
        "        \"\"\" Токенизируем текст \"\"\"\n",
        "        tokens = self._tokenize(text, append_eos_token)\n",
        "        ids = [self.vocab.get(token, self.vocab[self.unk_token]) for token in tokens]\n",
        "        return ids\n",
        "\n",
        "    def decode(self, input_ids: Iterable[int], remove_special_tokens: bool = False) -> str:\n",
        "        assert len(input_ids)\n",
        "        assert max(input_ids) < len(self.vocab) and min(input_ids) >= 0\n",
        "        tokens = []\n",
        "        for ind in input_ids:\n",
        "            token = self.inverse_vocab[ind]\n",
        "            if remove_special_tokens and token in self.special_tokens:\n",
        "                continue\n",
        "            tokens.append(token)\n",
        "        text = ' '.join( tokens )\n",
        "        return text\n",
        "\n",
        "    def save(self, path: str) -> bool:\n",
        "        data = {\n",
        "            'token_pattern': self.token_pattern,\n",
        "            'eos_token': self.eos_token,\n",
        "            'pad_token': self.pad_token,\n",
        "            'unk_token': self.unk_token,\n",
        "            'special_tokens': self.special_tokens,\n",
        "            'vocab': self.vocab,\n",
        "            'inverse_vocab': self.inverse_vocab,\n",
        "        }\n",
        "\n",
        "        with open(path, 'wb') as fout:\n",
        "            pickle.dump(data, fout)\n",
        "\n",
        "        return True\n",
        "\n",
        "    def load(self, path: str) -> bool:\n",
        "        with open(path, 'rb') as fin:\n",
        "            data = pickle.load(fin)\n",
        "\n",
        "        self.token_pattern = data['token_pattern']\n",
        "        self.eos_token = data['eos_token']\n",
        "        self.pad_token = data['pad_token']\n",
        "        self.unk_token = data['unk_token']\n",
        "        self.special_tokens = data['special_tokens']\n",
        "        self.vocab = data['vocab']\n",
        "        self.inverse_vocab = data['inverse_vocab']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8NjGu5pwQ1sL"
      },
      "outputs": [],
      "source": [
        "class GenerationConfig:\n",
        "    def __init__(self, **kwargs):\n",
        "        \"\"\"\n",
        "        Тут можно задать любые параметры и их значения по умолчанию\n",
        "        Значения для стратегии декодирования decoding_strategy: ['max', 'top-p']\n",
        "        \"\"\"\n",
        "        self.temperature = kwargs.pop(\"temperature\", 1.0)\n",
        "        self.max_tokens = kwargs.pop(\"max_tokens\", 32)\n",
        "        self.sample_top_p = kwargs.pop(\"sample_top_p\", 0.9)\n",
        "        self.decoding_strategy = kwargs.pop(\"decoding_strategy\", 'max')\n",
        "        self.remove_special_tokens = kwargs.pop(\"remove_special_tokens\", False)\n",
        "        self.validate()\n",
        "\n",
        "    def validate(self):\n",
        "        \"\"\" Здесь можно валидировать параметры \"\"\"\n",
        "        if not (1.0 > self.sample_top_p > 0):\n",
        "            raise ValueError('sample_top_p')\n",
        "        if self.decoding_strategy not in ['max', 'top-p']:\n",
        "            raise ValueError('decoding_strategy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDwYyOU_RVKZ"
      },
      "outputs": [],
      "source": [
        "class StatLM:\n",
        "    def __init__(self,\n",
        "                 tokenizer: Tokenizer,\n",
        "                 context_size: int = 2,\n",
        "                 alpha: float = 0.1\n",
        "                ):\n",
        "\n",
        "        assert context_size >= 2\n",
        "\n",
        "        self.context_size = context_size\n",
        "        self.tokenizer = tokenizer\n",
        "        self.alpha = alpha\n",
        "\n",
        "        self.n_gramms_stat = defaultdict(int)\n",
        "        self.nx_gramms_stat = defaultdict(int)\n",
        "\n",
        "    def get_token_by_ind(ind: int) -> str:\n",
        "        return self.tokenizer.vocab.get(ind)\n",
        "\n",
        "    def get_ind_by_token(token: str) -> int:\n",
        "        return self.tokenizer.inverse_vocab.get(token, self.tokenizer.inverse_vocab[self.unk_token])\n",
        "\n",
        "    def train(self, train_texts: List[str]):\n",
        "        for sentence in tqdm(train_texts, desc='train lines'):\n",
        "            sentence_ind = self.tokenizer.encode(sentence)\n",
        "            for i in range(len(sentence_ind) - self.context_size):\n",
        "\n",
        "                seq = tuple(sentence_ind[i: i + self.context_size - 1])\n",
        "                self.n_gramms_stat[seq] += 1\n",
        "\n",
        "                seq_x = tuple(sentence_ind[i: i + self.context_size])\n",
        "                self.nx_gramms_stat[seq_x] += 1\n",
        "\n",
        "            seq = tuple(sentence_ind[len(sentence_ind) - self.context_size:])\n",
        "            self.n_gramms_stat[seq] += 1\n",
        "\n",
        "    def sample_token(self,\n",
        "                     token_distribution: np.ndarray,\n",
        "                     generation_config: GenerationConfig) -> int:\n",
        "        if generation_config.decoding_strategy == 'max':\n",
        "            return token_distribution.argmax()\n",
        "        elif generation_config.decoding_strategy == 'top-p':\n",
        "            token_distribution = sorted(list(zip(token_distribution, np.arange(len(token_distribution)))),\n",
        "                                        reverse=True)\n",
        "            total_proba = 0.0\n",
        "            tokens_to_sample = []\n",
        "            tokens_probas = []\n",
        "            for token_proba, ind in token_distribution:\n",
        "                tokens_to_sample.append(ind)\n",
        "                tokens_probas.append(token_proba)\n",
        "                total_proba += token_proba\n",
        "                if total_proba >= generation_config.sample_top_p:\n",
        "                    break\n",
        "            # для простоты отнормируем вероятности, чтобы суммировались в единицу\n",
        "            tokens_probas = np.array(tokens_probas) / generation_config.temperature\n",
        "            tokens_probas = tokens_probas / tokens_probas.sum()\n",
        "            return np.random.choice(tokens_to_sample, p=tokens_probas)\n",
        "        else:\n",
        "            raise ValueError(f'Unknown decoding strategy: {generation_config.decoding_strategy}')\n",
        "\n",
        "    def save_stat(self, path: str) -> bool:\n",
        "        stat = {\n",
        "            'n_gramms_stat': self.n_gramms_stat,\n",
        "            'nx_gramms_stat': self.nx_gramms_stat,\n",
        "            'context_size': self.context_size,\n",
        "            'alpha': self.alpha\n",
        "        }\n",
        "        with open(path, 'wb') as fout:\n",
        "            pickle.dump(stat, fout)\n",
        "\n",
        "        return True\n",
        "\n",
        "    def load_stat(self, path: str) -> bool:\n",
        "        with open(path, 'rb') as fin:\n",
        "            stat = pickle.load(fin)\n",
        "\n",
        "        self.n_gramms_stat = stat['n_gramms_stat']\n",
        "        self.nx_gramms_stat = stat['nx_gramms_stat']\n",
        "        self.context_size = stat['context_size']\n",
        "        self.alpha = stat['alpha']\n",
        "\n",
        "        return True\n",
        "\n",
        "    def get_stat(self) -> Dict[str, Dict]:\n",
        "\n",
        "        n_token_stat, nx_token_stat = {}, {}\n",
        "        for token_inds, count in self.n_gramms_stat.items():\n",
        "            n_token_stat[self.tokenizer.decode(token_inds)] = count\n",
        "\n",
        "        for token_inds, count in self.nx_gramms_stat.items():\n",
        "            nx_token_stat[self.tokenizer.decode(token_inds)] = count\n",
        "\n",
        "        return {\n",
        "            'n gramms stat': self.n_gramms_stat,\n",
        "            'n+1 gramms stat': self.nx_gramms_stat,\n",
        "            'n tokens stat': n_token_stat,\n",
        "            'n+1 tokens stat': nx_token_stat,\n",
        "        }\n",
        "\n",
        "    def _get_next_token(self,\n",
        "                        tokens: List[int],\n",
        "                        generation_config: GenerationConfig) -> (int, str):\n",
        "        denominator = self.n_gramms_stat.get(tuple(tokens), 0) + self.alpha * len(self.tokenizer.vocab)\n",
        "        numerators = []\n",
        "        for ind in self.tokenizer.inverse_vocab:\n",
        "            numerators.append(self.nx_gramms_stat.get(tuple(tokens + [ind]), 0) + self.alpha)\n",
        "\n",
        "        token_distribution = np.array(numerators) / denominator\n",
        "        max_proba_ind = self.sample_token(token_distribution, generation_config)\n",
        "\n",
        "        next_token = self.tokenizer.inverse_vocab[max_proba_ind]\n",
        "\n",
        "        return max_proba_ind, next_token\n",
        "\n",
        "    def generate_token(self,\n",
        "                       text: str,\n",
        "                       generation_config: GenerationConfig\n",
        "                      ) -> Dict:\n",
        "        tokens = self.tokenizer.encode(text, append_eos_token=False)\n",
        "        tokens = tokens[-self.context_size + 1:]\n",
        "\n",
        "        max_proba_ind, next_token = self._get_next_token(tokens, generation_config)\n",
        "\n",
        "        return {\n",
        "            'next_token': next_token,\n",
        "            'next_token_num': max_proba_ind,\n",
        "        }\n",
        "\n",
        "\n",
        "    def generate_text(self, text: str,\n",
        "                      generation_config: GenerationConfig\n",
        "                     ) -> Dict:\n",
        "\n",
        "        all_tokens = self.tokenizer.encode(text, append_eos_token=False)\n",
        "        tokens = all_tokens[-self.context_size + 1:]\n",
        "\n",
        "        next_token = None\n",
        "        while next_token != self.tokenizer.eos_token and len(all_tokens) < generation_config.max_tokens:\n",
        "            max_proba_ind, next_token = self._get_next_token(tokens, generation_config)\n",
        "            all_tokens.append(max_proba_ind)\n",
        "            tokens = all_tokens[-self.context_size + 1:]\n",
        "\n",
        "        new_text = self.tokenizer.decode(all_tokens, generation_config.remove_special_tokens)\n",
        "\n",
        "        finish_reason = 'max tokens'\n",
        "        if all_tokens[-1] == self.tokenizer.vocab[self.tokenizer.eos_token]:\n",
        "            finish_reason = 'end of text'\n",
        "\n",
        "        return {\n",
        "            'all_tokens': all_tokens,\n",
        "            'total_text': new_text,\n",
        "            'finish_reason': finish_reason\n",
        "        }\n",
        "\n",
        "    def generate(self, text: str, generation_config: Dict) -> str:\n",
        "        return self.generate_text(text, generation_config)['total_text']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CmY1gzT3RXlS"
      },
      "outputs": [],
      "source": [
        "def construct_model():\n",
        "    config = {\n",
        "        'temperature': 1.0,\n",
        "        'max_tokens': 32,\n",
        "        'sample_top_p': 0.9,\n",
        "        'decoding_strategy': 'top-p',\n",
        "    }\n",
        "\n",
        "    stat_lm_path = 'models/stat_lm/stat_lm.pkl'\n",
        "    tokenizer_path = 'models/stat_lm/tokenizer.pkl'\n",
        "\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.load(tokenizer_path)\n",
        "\n",
        "    stat_lm = StatLM(tokenizer)\n",
        "    stat_lm.load_stat(stat_lm_path)\n",
        "\n",
        "    generation_config = GenerationConfig(temperature=config['temperature'],\n",
        "                                         max_tokens=config['max_tokens'],\n",
        "                                         sample_top_p=config['sample_top_p'],\n",
        "                                         decoding_strategy=config['decoding_strategy'],\n",
        "                                         remove_special_tokens=True)\n",
        "\n",
        "    kwargs = {'generation_config': generation_config}\n",
        "    return stat_lm, kwargs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDCZa-fHUb9d"
      },
      "source": [
        "https://huggingface.co/datasets/Den4ikAI/russian_dialogues\n",
        "https://huggingface.co/datasets/Georgii/russianPoetry\n",
        "https://huggingface.co/datasets/IgorVolochay/russian_jokes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DI9tgnwPgS5R"
      },
      "outputs": [],
      "source": [
        "df=pd.read_csv('dataset2.txt', delimiter='\\t')\n",
        "text_dm=df.values.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8okOa-yTq-i2"
      },
      "outputs": [],
      "source": [
        "text=[]\n",
        "for i in text_dm:\n",
        " text.append(i[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCAvMgZwg72R",
        "outputId": "c4762794-97be-41ce-e96e-0405a3e26920"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6oM8_AiZlXTq"
      },
      "outputs": [],
      "source": [
        "train_texts = text[:-1]\n",
        "test_text = text[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t72EoPpomIJ0"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer().build_vocab(train_texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lzgVQVSamRSw",
        "outputId": "6599c61e-3bd6-4cd1-b3f3-6ad7438c6837"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'буйнопомешанные': 0,\n",
              " 'болтает': 1,\n",
              " 'физическую': 2,\n",
              " 'кабинка': 3,\n",
              " 'кaкoвa': 4}"
            ]
          },
          "metadata": {},
          "execution_count": 225
        }
      ],
      "source": [
        "dict(list(tokenizer.vocab.items())[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "27261f86fe8840809ed786f0d077bde4",
            "c1f075300a6e4a1885b7684ed3857674",
            "3f862b23b9c242ae969e85183156cb36",
            "62bc53447e6b4e1b8eb5d69169a62e94",
            "c3b9839e4ffb44d7931c83fcaf9fb764",
            "3e174b1f2dac4527a615e9c4b86f1d5d",
            "83d65f6e1eff4ca0acc3ea5df08696bd",
            "6cf0e23625134d049b4dcb13307fff0c",
            "7d81cc9dc8a64bf5aaf3f4ffbde05285",
            "e45c2b5f2f3a4fec86e8a3743ceca74f",
            "0d508fa1e0ed40e59a6ca869cab36d23"
          ]
        },
        "id": "202mFQGpmSeQ",
        "outputId": "1e46c158-8ad8-4785-a045-9f4123c9e375"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train lines:   0%|          | 0/150551 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "27261f86fe8840809ed786f0d077bde4"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# класс, который позволяем строить и использовать языковую модель на основе n-грамм\n",
        "stat_lm = StatLM(tokenizer, context_size=3, alpha=0.3) # , sample_top_p = None\n",
        "\n",
        "# \"обучаем\" модель - считаем статистики\n",
        "stat_lm.train(train_texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rvy-nme35z5A"
      },
      "outputs": [],
      "source": [
        "generation_config = GenerationConfig(temperature = 0.8, max_tokens = 12,\n",
        "                                     sample_top_p = 0.9, decoding_strategy = 'max',\n",
        "                                     remove_special_tokens=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2STyO8wmW7h",
        "outputId": "a44a14c2-1913-4ff5-9f03-f2ef610cc0f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "где ленин ? - да , но не все , что я\n"
          ]
        }
      ],
      "source": [
        "print(stat_lm.generate(\"где Ленин? \", generation_config))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FfMqTrCpmaIM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b759bae8-84fe-4b51-bc32-1d7ed673d3dd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 330
        }
      ],
      "source": [
        "tokenizer.save('/content/tokenizer.pkl')\n",
        "stat_lm.save_stat('/content/stat_lm.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iXg-U5o4meiB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "outputId": "cf6b9f3c-a56f-4a4f-fc0d-540da45465de"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'models/stat_lm/tokenizer.pkl'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-331-2e01aa11d76b>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstruct_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"дошик\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-208-cfba78483a17>\u001b[0m in \u001b[0;36mconstruct_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mstat_lm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStatLM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-205-23afe828bd16>\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'models/stat_lm/tokenizer.pkl'"
          ]
        }
      ],
      "source": [
        "model, kwargs = construct_model()\n",
        "\n",
        "model.generate(\"дошик\", **kwargs)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "27261f86fe8840809ed786f0d077bde4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c1f075300a6e4a1885b7684ed3857674",
              "IPY_MODEL_3f862b23b9c242ae969e85183156cb36",
              "IPY_MODEL_62bc53447e6b4e1b8eb5d69169a62e94"
            ],
            "layout": "IPY_MODEL_c3b9839e4ffb44d7931c83fcaf9fb764"
          }
        },
        "c1f075300a6e4a1885b7684ed3857674": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3e174b1f2dac4527a615e9c4b86f1d5d",
            "placeholder": "​",
            "style": "IPY_MODEL_83d65f6e1eff4ca0acc3ea5df08696bd",
            "value": "train lines: 100%"
          }
        },
        "3f862b23b9c242ae969e85183156cb36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6cf0e23625134d049b4dcb13307fff0c",
            "max": 150551,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7d81cc9dc8a64bf5aaf3f4ffbde05285",
            "value": 150551
          }
        },
        "62bc53447e6b4e1b8eb5d69169a62e94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e45c2b5f2f3a4fec86e8a3743ceca74f",
            "placeholder": "​",
            "style": "IPY_MODEL_0d508fa1e0ed40e59a6ca869cab36d23",
            "value": " 150551/150551 [00:21&lt;00:00, 5855.61it/s]"
          }
        },
        "c3b9839e4ffb44d7931c83fcaf9fb764": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e174b1f2dac4527a615e9c4b86f1d5d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83d65f6e1eff4ca0acc3ea5df08696bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6cf0e23625134d049b4dcb13307fff0c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d81cc9dc8a64bf5aaf3f4ffbde05285": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e45c2b5f2f3a4fec86e8a3743ceca74f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d508fa1e0ed40e59a6ca869cab36d23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}